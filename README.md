# ğŸ“Š AWS Cloud-Based ETL Pipeline

A complete serverless ETL (Extract, Transform, Load) pipeline built using **AWS Lambda**, **S3**, and **Amazon Redshift**, provisioned with **Terraform** and powered by **Python**.

---

## Project Structure
```bash
ETL_Design/
â”œâ”€â”€ data/ # Local test data files (CSV, JSON)
â”œâ”€â”€ infrastructure/ # Terraform infrastructure as code
â”‚ â”œâ”€â”€ redshift.tf
â”‚ â”œâ”€â”€ lambda.tf
â”‚ â”œâ”€â”€ s3.tf
â”‚ â”œâ”€â”€ iam.tf
â”‚ â”œâ”€â”€ stepfunctions.tf
â”‚ â”œâ”€â”€ outputs.tf
â”‚ â”œâ”€â”€ vars.tf
â”‚ â””â”€â”€ providers.tf
â”œâ”€â”€ lambda_functions/ # Lambda transformation logic
â”‚ â”œâ”€â”€ functions.py
â”‚ â”œâ”€â”€ Dockerfile
â”‚ â”œâ”€â”€ build_and_push.sh
â”‚ â””â”€â”€ requirements.txt
â”œâ”€â”€ move_data_to_s3.py # Uploads raw files to S3
â”œâ”€â”€ load.py # Loads transformed files into Redshift staging
â”œâ”€â”€ create_staging_tables.sql # SQL for Redshift staging tables
â”œâ”€â”€ create_final_tables.sql # SQL for Redshift final tables and views
â”œâ”€â”€ execute_staging_sql.py # Executes staging DDL
â”œâ”€â”€ execute_final_sql.py # Executes final DDL and view logic
â”œâ”€â”€ .env # Stores environment variables (excluded from Git)
â”œâ”€â”€ .gitignore # Git ignore config
â””â”€â”€ README.md
```

## What It Does

1. **Extract** raw data from CSV and JSON in the `data/` folder
2. **Transform** using AWS Lambda (cleaning, merging, validation, enrichment)
3. **Load** transformed `.parquet` files into Amazon Redshift staging tables
4. **Insert** cleaned data into final tables with foreign key resolution
5. **Create** views (data marts) for analytics

---

## Technologies Used

- **AWS S3** â€“ Storage for raw and transformed data
- **AWS Lambda (Dockerized)** â€“ ETL logic execution
- **Amazon Redshift Serverless** â€“ Data warehouse
- **Terraform** â€“ Infrastructure provisioning
- **Python (pandas, boto3)** â€“ Data transformation and orchestration
- **Parquet** â€“ Efficient data format for Redshift Spectrum

---

## Views Created

- `v_sales_per_store` â€“ Total sales per store  
- `v_top_products_by_quantity` â€“ Top 4 most-sold products  
- `v_employee_count_per_store` â€“ Employee distribution per store  
- `v_revenue_per_city` â€“ Total revenue by customer city  

---

## How to Run Locally

1. **Upload raw data to S3**  
   ```bash
   python move_data_to_s3.py
   
2. **Run Lambda ETL (triggered by AWS Step Functions or manually)3**  
   
3. **Load transformed data into Redshift from the root directory**  
   ```bash
   python load.py

4. **Create staging/final tables and views from the root directory**  
   ```bash
   python execute_staging_sql.py 
   python execute_final_sql.py

## Environment Variables (.env)
- âš ï¸ Add this file to `.gitignore` to keep credentials secure.

## Git Ignore (.gitignore)
```bash
.env
*.parquet
__pycache__/
.terraform/
.DS_Store
*.zip
```
## Terraform Usage
```bash
cd infrastructure/
terraform init
terraform apply
```
Make sure your AWS CLI is authenticated (aws configure)

## Author
Prosper Amamgbo

Data Engineer

https://github.com/Prosper-Ama



   
